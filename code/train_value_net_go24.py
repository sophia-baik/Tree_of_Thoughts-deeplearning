# -*- coding: utf-8 -*-
"""game_of_24_value_function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVV8nJmv8L6oV0ej-5Hmm-UBXqrqVDIb

Goal: Find value function 0 <= V(y) <= 1 where V(y) is the estimated probability
that we will reach the goal (24) from state y
Creates weights for a neural network.
"""

import ast
import pandas as pd
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from itertools import combinations

data_path = "gameof24/24game_problems.csv"

df = pd.read_csv(data_path)
game_of_24_quads = [ast.literal_eval(x) for x in df['numbers']]
"""
from google.colab import files
game_of_24_data = files.upload()
df = pd.read_csv(next(iter(game_of_24_data.keys())))
game_of_24_quads = [ast.literal_eval(x) for x in df['numbers']]
"""

def simulate_paths(quad, max_depth = 3):
  """
  Simulates all possible paths from a given set of numbers by applying all pairwise operations.
  Returns a dictionary mapping states to scores (normalized values).
  
  Leaf nodes that can reach 24 get a score of 1.0, parents get 0.75, and grandparents 0.5.
  """
  labels = {} # key:state, value: score
  path_stack = []

  def dfs(state, depth):
    # Sort the state to ensure the order doesn't affect the result
    state_sorted = tuple(sorted(float(x) for x in state))
    path_stack.append(state_sorted)

    if state_sorted not in labels:
      labels[state_sorted] = 0

    if len(state) == 1:
      if abs(state[0] - 24) < 1e-6:
        reward = 1.0
        # Update rewards for all ancestors in the path
        for idx, s in enumerate(reversed(path_stack)):
            reward = max(0, reward - 0.25)  # Decrease reward with depth
            labels[s] = labels.get(s, 0.0) + reward
      path_stack.pop()
      return

    # Generate new states by applying all possible operations
    for (i, j) in combinations(range(len(state)), 2):
      a, b = state[i], state[j]

      remaining_numbers = [state[k] for k in range(len(state)) if k != i and k != j]
      candidates = [a + b, a - b, b - a, a * b]
      if a != 0:
        candidates.append(b / a)
      if b != 0:
        candidates.append(a / b)

      for new_num in candidates:
        new_state = remaining_numbers + [new_num]
        dfs(new_state, depth - 1)

    path_stack.pop()

  dfs(quad, max_depth)

  # Normalize scores
  max_val = max(labels.values(), default = 1)
  for s in labels:
      labels[s] /= max_val
  return labels

def generate_training_data(data, max_depth = 3):
  """
  Generates training data by simulating paths for each initial state.
  Returns feature vectors and corresponding values.
  """
  X, Y = [], []
  for quad in data:
    reachable_states = simulate_paths(quad, max_depth)
    for state, value in reachable_states.items():
      X.append(pad(state))
      Y.append(value)
  return X, Y

def pad(state):
  """
  Pads the state with zeroes to ensure it always has length 4.
  """
  return [0] * (4 - len(state)) + list(state)

def add_noise(states, values, noise_level=0.05, num_augments=3):
    """
    Augments the dataset by adding noise to the states.
    Each state will be augmented `num_augments` times.
    """
    new_states, new_values = [], []
    for state, value in zip(states, values):
        new_states.append(state)
        new_values.append(value)
        for _ in range(num_augments):
            noisy_state = [
                max(0, x + random.uniform(-noise_level, noise_level)) for x in state
            ]
            new_states.append(noisy_state)
            new_values.append(value)
    return new_states, new_values

def oversample(states, values, target_ratio=0.5):
  """
  Performs oversampling to balance the classes, targeting a specific ratio of positive values.
  """
  pos = [(s, v) for s, v in zip(states, values) if v > 0]
  neg = [(s, v) for s, v in zip(states, values) if v == 0]

  pos_count = len(pos)
  neg_count = len(neg)

  desired_pos_count = int((target_ratio * neg_count) / (1 - target_ratio))
  multiplier = max(1, desired_pos_count // pos_count)

  oversampled_pos = pos * multiplier
  combined = oversampled_pos + neg
  random.shuffle(combined)

  final_states, final_values = zip(*combined)
  return list(final_states), list(final_values)

class ValueNetwork(nn.Module):
    """
    MLP model to predict the value of a state (probability of reaching 24).
    """
    def __init__(self, input_size=37, hidden_sizes=[128, 128, 64], dropout_rates=[0.3, 0.3, 0.2]):
      super(ValueNetwork, self).__init__()
      
      self.fc1 = nn.Linear(input_size, hidden_sizes[0])
      self.ln1 = nn.LayerNorm(hidden_sizes[0])
      self.dropout1 = nn.Dropout(dropout_rates[0])
      
      self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
      self.ln2 = nn.LayerNorm(hidden_sizes[1])
      self.dropout2 = nn.Dropout(dropout_rates[1])
      
      self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])
      self.ln3 = nn.LayerNorm(hidden_sizes[2])
      self.dropout3 = nn.Dropout(dropout_rates[2])
      
      self.fc4 = nn.Linear(hidden_sizes[2], 1)
      
      self.skip_connection = nn.Linear(hidden_sizes[0], hidden_sizes[1]) if hidden_sizes[0] != hidden_sizes[1] else nn.Identity()

    def forward(self, x):
      layer1 = F.leaky_relu(self.ln1(self.fc1(x)), negative_slope = 0.01)
      layer1 = self.dropout1(layer1)

      layer2_pre = self.ln2(self.fc2(layer1))
      skip = self.skip_connection(layer1)
      layer2 = F.leaky_relu(layer2_pre + skip, negative_slope=0.01)
      layer2 = self.dropout2(layer2)     
      
      layer3 = F.leaky_relu(self.ln3(self.fc3(layer2)), negative_slope=0.01)
      layer3 = self.dropout3(layer3)

      return self.fc4(layer3)

def train_value_network(X_train, y_train, epochs=10, batch_size=32):
    """
    Trains the value network using binary cross-entropy loss.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ValueNetwork().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.BCELoss()

    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)

    sample_weights = torch.where(y_train > 0, torch.tensor(60.0).to(device), torch.tensor(1.0)).to(device)

    for epoch in range(epochs):
      total_loss = 0
      total_mae = 0
      perm = torch.randperm(len(X_train))
      for i in range(0, len(X_train), batch_size):
          idx = perm[i:i+batch_size]
          batch_x = X_train[idx]
          batch_y = y_train[idx]
          batch_weights = sample_weights[idx]

          optimizer.zero_grad()
          preds = model(batch_x)
          loss = (batch_weights * loss_fn (preds, batch_y) ** 2).mean()
          loss.backward()
          optimizer.step()

          total_loss += loss.item() * batch_x.size(0)
          total_mae += torch.abs(preds - batch_y).sum().item()

      avg_loss = total_loss / len(X_train)
      avg_mae = total_mae / len(X_train)
      print(f"Epoch {epoch+1}: Loss = {avg_loss:.4f}, MAE = {avg_mae:.4f}")

    return model

if __name__ == "__main__":
  # Generate training data and apply data augmentation + oversampling
  states, values = generate_training_data(game_of_24_quads)
  states, values = add_noise(states, values)
  states, values = oversample(states, values, target_ratio=0.5)
  # Train the model and save the weights
  model = train_value_network(states, values, epochs=10, batch_size=32)
  torch.save(model.state_dict(), 'trained_value_net.pth')