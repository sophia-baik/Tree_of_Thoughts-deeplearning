# -*- coding: utf-8 -*-
"""game_of_24_value_function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVV8nJmv8L6oV0ej-5Hmm-UBXqrqVDIb

Goal: Find value function 0 <= V(y) <= 1 where V(y) is the estimated probability
that we will reach the goal (24) from state y

We want to create a dataset of partial states with values:
*  0 (can't reach 24)
*  1 (can reach 24)

MLP expects fixed sized input so we pad our feature space with zeroes, maybe append max/mean as additional features

Loss: binary cross entropy
"""

import itertools
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
import numpy as np
import ast
from typing import List
from torch.utils.data import DataLoader, TensorDataset
from util_gameof24 import extract_features
from sklearn.metrics import f1_score, precision_score, recall_score,precision_recall_fscore_support

data_path = "gameof24/24game_problems.csv"

df = pd.read_csv(data_path)
game_of_24_quads = [ast.literal_eval(x) for x in df['numbers']]

def simulate_paths(quad, max_depth = 3):
  """
  Simulates all possible actions that the LLM can take to combine numbers.
  Encompasses all pairs of numbers and all operations.
  """
  visited = set()
  success = set()

  def dfs(state, depth, path):
    # sorted ensures that states are order agnostic and we aren't learning different
    # weights for states that are logically the same
    state = sorted(float(x) for x in state)

    if tuple(state) in visited:
      return
    visited.add(tuple(state))
    path.append(state)

    if len(state) == 1:
      # if we're using floats we need to allow some wiggle room
      if abs(state[0] - 24) < 1e-6:
        # mark parents as success (this is why we're doing dfs not bfs)
        for s in path:
          success.add((tuple(s), True))
        return

    if depth == 0:
      success.add((tuple(state), False))
      return

    # get all pairs of numbers
    for (i, j) in itertools.combinations(range(len(state)), 2):
      a, b = state[i], state[j]

      candidates = [a + b, a - b, b - a, a * b]
      if a != 0:
        candidates.append(b / a)
      if b != 0:
        candidates.append(a / b)

      for new_num in candidates:
        # create a new num using the created candidate and the other nums we didn't use
        new_state = [state[k] for k in range(len(state)) if k != i and k != j] + [new_num]
        dfs(new_state, depth-1, path)

  dfs(quad, max_depth,[])
  return success

def generate_training_data(data, max_depth = 3):
  X = []
  Y = []

  for quad in data:
    reachable_states = simulate_paths(quad, max_depth)
    for state, value in reachable_states:
      X.append(extract_features(state))  
      Y.append(int(value))
  return X, Y

class ValueNetwork(nn.Module):
  def __init__(self, input_size=37, hidden_sizes=[128, 128, 64], dropout_rates=[0.3, 0.3, 0.2]):
        super(ValueNetwork, self).__init__()
        
        self.fc1 = nn.Linear(input_size, hidden_sizes[0])
        self.ln1 = nn.LayerNorm(hidden_sizes[0])
        self.dropout1 = nn.Dropout(dropout_rates[0])
        
        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
        self.ln2 = nn.LayerNorm(hidden_sizes[1])
        self.dropout2 = nn.Dropout(dropout_rates[1])
        
        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])
        self.ln3 = nn.LayerNorm(hidden_sizes[2])
        self.dropout3 = nn.Dropout(dropout_rates[2])
        
        self.fc4 = nn.Linear(hidden_sizes[2], 1)
        
        self.skip_connection = nn.Linear(hidden_sizes[0], hidden_sizes[1]) if hidden_sizes[0] != hidden_sizes[1] else nn.Identity()

  def forward(self, x):
      layer1 = F.leaky_relu(self.ln1(self.fc1(x)), negative_slope = 0.01)
      layer1 = self.dropout1(layer1)

      layer2_pre = self.ln2(self.fc2(layer1))
      skip = self.skip_connection(layer1)
      layer2 = F.leaky_relu(layer2_pre + skip, negative_slope=0.01)
      layer2 = self.dropout2(layer2)     
      
      layer3 = F.leaky_relu(self.ln3(self.fc3(layer2)), negative_slope=0.01)
      layer3 = self.dropout3(layer3)

      return self.fc4(layer3)

def train_value_network(X_train, y_train, epochs=10, batch_size=32):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)

    dataset = TensorDataset(X_train, y_train)
    loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)

    model = ValueNetwork().to(device)

    pos_weight_value = (len(y_train) - y_train.sum()) / (y_train.sum() + 1e-6)
    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_value.to(device))

    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

    for epoch in range(epochs):
      model.train()
      total_loss = 0
      correct = 0
      total = 0
      all_preds = []
      all_probs = []
      all_labels = []

      for batch_x, batch_y in loader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
      
        optimizer.zero_grad()
        logits = model(batch_x)
        loss = loss_fn(logits, batch_y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)
        preds = (torch.sigmoid(logits) > 0.5).float()
        correct += (preds == batch_y).sum().item()
        total += batch_x.size(0)

        probs = torch.sigmoid(logits).detach().cpu().numpy()
        labels = batch_y.detach().cpu().numpy()
        binary_preds = (probs > 0.4).astype(int)

        all_probs.extend(probs.flatten())
        all_preds.extend(binary_preds.flatten())
        all_labels.extend(labels.flatten())

        correct += (binary_preds.flatten() == labels.flatten()).sum()
        total += batch_x.size(0)

      avg_loss = total_loss / total
      accuracy = correct / total
      f1 = f1_score(all_labels, all_preds)
      precision = precision_score(all_labels, all_preds)
      recall = recall_score(all_labels, all_preds)

      # Finds the best threshold to use for our Binary Cross Entropy Loss
      # ("\nThreshold Sweep:")
      # for thresh in np.arange(0.1, 0.9, 0.1):
      #     binary_preds = (np.array(all_probs) > thresh).astype(int)
      #     p, r, f, _ = precision_recall_fscore_support(all_labels, binary_preds, average='binary', zero_division=0)
      #     print(f"Thresh={thresh:.2f} | F1: {f:.3f}, Precision: {p:.3f}, Recall: {r:.3f}")

      print(f"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}, "
            f"F1 = {f1:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}")

      scheduler.step(avg_loss)

    return model

if __name__ == "__main__":
    df = pd.read_csv("gameof24/24game_problems.csv")
    game_of_24_quads = [ast.literal_eval(x) for x in df['numbers']]

    States, Values = generate_training_data(game_of_24_quads)
    print(f"Positive rate: {sum(Values)/len(Values):.4f}")
    model = train_value_network(States, Values, epochs=10, batch_size=256)
    torch.save(model.state_dict(), 'value_network.pth')